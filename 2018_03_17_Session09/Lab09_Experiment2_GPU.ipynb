{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab09-Experiment2-GPU.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/apparaonali/AIML/blob/master/2018_03_17_Session09/Lab09_Experiment2_GPU.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Xzn9l36i_hGQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system('pip3 install seaborn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qGaAmPRD_hGV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Importing required packages\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "### Importing torch packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "## Importing python packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "### To test whether GPU instance is present in the system of not.\n",
        "cuda = torch.cuda.is_available()\n",
        "print('Using PyTorch version:', torch.__version__, 'CUDA:', cuda)\n",
        "### If cuda is a gpu instance. If it's false then we run the program on CPU\n",
        "### If cuda is a gpu instance. If it's true then we run the program on GPU\n",
        "torch.manual_seed(42)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i4OEioUT_hGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we'll load the MNIST data. First time we may have to download the data, which can take a while."
      ]
    },
    {
      "metadata": {
        "id": "alxjaxNq_hGY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Initializing batch size\n",
        "batch_size = 32\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "\n",
        "## Loading the train set file\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "## Loading the test set file\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHSRVWAD_hGb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The train and test data are provided via data loaders that provide iterators over the datasets. The first element of training data (X_train) is a 4th-order tensor of size (batch_size, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels. y_train is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training digit."
      ]
    },
    {
      "metadata": {
        "id": "Rm_yRZfN_hGc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for (X_train, y_train) in train_loader:\n",
        "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
        "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hvk661Ff_hGg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Plotting the  first 10 training digits"
      ]
    },
    {
      "metadata": {
        "id": "fejdBEuZ_hGg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pltsize=1\n",
        "plt.figure(figsize=(10*pltsize, pltsize))\n",
        "\n",
        "for i in range(10):\n",
        "    plt.subplot(1,10,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray\")\n",
        "    plt.title('Class: '+str(y_train[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CEJ6gW_n_hGi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### MLP network definition\n",
        "Let's define the network as a Python class. We have to write the __init__() and forward() methods, and PyTorch will automatically generate a backward() method for computing the gradients for the backward pass.\n",
        "\n",
        "Finally, we define an optimizer to update the model parameters based on the computed gradients. We select stochastic gradient descent (with momentum) as the optimization algorithm, and set learning rate to 0.01. Note that there are several different options for the optimizer in PyTorch that we could use instead of SGD."
      ]
    },
    {
      "metadata": {
        "id": "ruYSiB8U_hGk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 50)\n",
        "        self.fc1_drop = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.fc2_drop = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc1_drop(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc2_drop(x)\n",
        "        return F.log_softmax(self.fc3(x))\n",
        "\n",
        "model = Net()\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    \n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VzF9NU13_hGm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Learning\n",
        "Let's now define functions to train() and test() the model."
      ]
    },
    {
      "metadata": {
        "id": "WVLHZcWd_hGn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epoch, log_interval=100):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.data[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y9SRhfdj_hGq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(loss_vector, accuracy_vector):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    for data, target in test_loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target).data[0]\n",
        "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    loss_vector.append(test_loss)\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    accuracy_vector.append(accuracy)\n",
        "    \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nge32uwL_hGt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train our model using the train() function. An epoch means one pass through the whole training data. After each epoch, we evaluate the model using test()."
      ]
    },
    {
      "metadata": {
        "id": "hyLZcBfr_hGt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.system('date')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gq2ZT37B_hGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a6zV7uOL_hG0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.system('date')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}